{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirwarobert/cnn/blob/main/ISTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R2UjiHBZyEVg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports the necessary libraries for building and working with deep learning models using TensorFlow and Keras. The first line imports TensorFlow as tf, which is a popular open-source framework for machine learning and deep learning, providing powerful tools for building and training models. The second line imports Keras from TensorFlow, which is a high-level neural networks API that simplifies the process of building, training, and evaluating deep learning models. The third line imports NumPy as np, a library used for numerical computing in Python, which is particularly useful for handling arrays and performing mathematical operations. These libraries are commonly used together in machine learning tasks to create and manipulate models, process data, and perform numerical computations."
      ],
      "metadata": {
        "id": "1M4afjA2cp8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the buily-in IMDB dataset\n",
        "imdb=keras.datasets.imdb"
      ],
      "metadata": {
        "id": "l3seB-hP2KR8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads the built-in IMDB dataset from Keras, which is a widely-used dataset for binary sentiment classification tasks. The IMDB dataset contains movie reviews labeled with either positive or negative sentiment. By calling keras.datasets.imdb, Keras provides easy access to this dataset, which can then be used to train machine learning models for text classification. The dataset consists of 25,000 training reviews and 25,000 test reviews, each review being a sequence of integers representing words. This code doesn't explicitly load the data yet; rather, it makes the dataset available for use by referencing keras.datasets.imdb, which can be followed by commands to load, preprocess, and use the data."
      ],
      "metadata": {
        "id": "AHiAnFNEc3vW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the vocabulary size and maximum sequence length\n",
        "vocab_size=10000\n",
        "max_length=250"
      ],
      "metadata": {
        "id": "VHa15ydM83iq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines two important parameters for text preprocessing when working with the IMDB dataset. vocab_size=10000 sets the size of the vocabulary, meaning the model will consider the top 10,000 most frequent words in the dataset. Any word outside this vocabulary will be ignored or replaced with a special token. max_length=250 specifies the maximum length of each input sequence (in terms of the number of words). If a review has more than 250 words, it will be truncated to fit this length, and if it has fewer words, it will be padded with zeros to ensure uniformity in input size. These parameters help control the input size and vocabulary complexity, making the model more manageable and ensuring efficient processing."
      ],
      "metadata": {
        "id": "HdF54sXfdHMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC2UA-pU9Jpo",
        "outputId": "b5ee2fc4-7178-41f0-e03b-988a3165b5cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads the IMDB dataset for sentiment analysis using the imdb.load_data function, with the parameter num_words=vocab_size set to 10,000. This ensures that only the 10,000 most frequent words in the dataset are considered, and any words outside this vocabulary are excluded, helping to limit the model's complexity and focus on the most common terms. The dataset is split into training (x_train, y_train) and test (x_test, y_test) sets, where x_train and x_test contain the movie reviews (represented as sequences of integers corresponding to words), and y_train and y_test contain the labels, which are binary values indicating whether the review is positive (1) or negative (0). This setup prepares the data for training and evaluation in a sentiment analysis model."
      ],
      "metadata": {
        "id": "Z_qrmcM3dVcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences to have the same length\n",
        "x_train=keras.preprocessing.sequence.pad_sequences(x_train,maxlen=max_length)\n",
        "x_test=keras.preprocessing.sequence.pad_sequences(x_test,maxlen=max_length)"
      ],
      "metadata": {
        "id": "HyVGNIau9anY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code pads the sequences in the training and test sets to ensure that all input sequences have the same length, which is essential for training deep learning models. The keras.preprocessing.sequence.pad_sequences function is used to adjust the length of the review sequences (x_train and x_test) to a consistent size of max_length=250. If a review contains more than 250 words, it will be truncated, and if it contains fewer words, it will be padded with zeros at the beginning (by default) to reach the required length. Padding ensures that the input data is uniform in size, making it suitable for feeding into a neural network, which requires fixed-size input."
      ],
      "metadata": {
        "id": "Q8-nxqtrdlT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model\n",
        "model=keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size,32),\n",
        "    keras.layers.LSTM(32),\n",
        "    keras.layers.Dense(1,activation='sigmoid')\n",
        "  ])"
      ],
      "metadata": {
        "id": "I1auNmUT9_WI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code builds an LSTM (Long Short-Term Memory) model for sentiment analysis using the Keras Sequential API. The model consists of three layers: First, an Embedding layer is added with a vocabulary size of 10,000 (vocab_size) and an embedding dimension of 32, which converts the integer sequences of words into dense vectors of fixed size (32-dimensional). This allows the model to learn word representations in a continuous vector space. Next, an LSTM layer with 32 units is added to capture the temporal dependencies and patterns in the sequence of words, helping the model understand the context of the words in the reviews. Finally, a Dense layer with a single unit and a sigmoid activation function is added to produce the binary classification output (positive or negative sentiment), with a value between 0 and 1. This architecture is well-suited for sequence-based tasks like sentiment analysis, where the order and relationships between words are important."
      ],
      "metadata": {
        "id": "vWdLmWF1d27s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "               loss='binary_crossentrophy',\n",
        "               metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "aaLP6CGa-tKD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code compiles the LSTM model, preparing it for training by specifying the optimization algorithm, loss function, and evaluation metrics. The Adam optimizer is chosen for training, which is an adaptive learning rate optimization algorithm known for its efficiency and effectiveness in deep learning tasks. The binary cross-entropy loss function is used because this is a binary classification problem (sentiment analysis), where the model predicts either a positive or negative sentiment. Binary cross-entropy measures how well the predicted probabilities match the true binary labels. The model is also configured to track accuracy as a metric, which will report the percentage of correct predictions during training and evaluation. This compilation step defines how the model will update its weights during training and how its performance will be evaluated."
      ],
      "metadata": {
        "id": "1jjETjHXeDnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model\n",
        "model=keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size,32),\n",
        "    keras.layers.LSTM(32, input_shape=(x_train.shape[1], 32)), # Specify input_shape for LSTM\n",
        "    keras.layers.Dense(1,activation='sigmoid')\n",
        "  ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYCyOYcfB6Qe",
        "outputId": "121a9e07-ffe1-43e3-e76c-79e0372fec24"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code builds an LSTM model using the Keras Sequential API for binary sentiment analysis. The model begins with an Embedding layer that maps the input word indices (from the vocab_size of 10,000) into dense vectors of size 32, allowing the model to learn a continuous representation of words. Next, an LSTM layer with 32 units is added to capture the temporal dependencies in the sequences of words. The input_shape=(x_train.shape[1], 32) argument specifies the shape of the input data, where x_train.shape[1] is the sequence length (the number of words per review, typically padded to a fixed size), and 32 is the dimensionality of the word embeddings. Finally, a Dense layer with a single unit and a sigmoid activation function is used to output a probability value between 0 and 1, indicating the likelihood of a positive sentiment. This model is designed to predict whether a movie review has a positive or negative sentiment based on the sequence of words."
      ],
      "metadata": {
        "id": "wgk4eQXHeU9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "               loss='binary_crossentropy', # Corrected loss function name\n",
        "               metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "FMNyeC9gB2-L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code compiles the LSTM model, specifying the optimizer, loss function, and evaluation metrics for training. The Adam optimizer is selected due to its efficiency and effectiveness in handling large datasets and complex models by adjusting the learning rate dynamically during training. The binary cross-entropy loss function is used, which is appropriate for binary classification tasks like sentiment analysis, where the model outputs a probability that the review is either positive or negative. Binary cross-entropy calculates the difference between the true and predicted binary labels, helping the model minimize prediction errors. Additionally, the accuracy metric is included to track the proportion of correct predictions, providing a clear measure of the model’s performance during training and evaluation. This setup prepares the model for the training process."
      ],
      "metadata": {
        "id": "3JJ1Npl1ekLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(x_train,y_train,\n",
        "                  epochs=10,\n",
        "                  batch_size=32,\n",
        "                  validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dKPqk7pe636",
        "outputId": "f66f429c-01c2-4a53-c257-65be300bb385"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.9896 - loss: 0.0347 - val_accuracy: 0.8558 - val_loss: 0.5781\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9926 - loss: 0.0248 - val_accuracy: 0.8568 - val_loss: 0.6312\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.9936 - loss: 0.0208 - val_accuracy: 0.8532 - val_loss: 0.6462\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.9920 - loss: 0.0241 - val_accuracy: 0.8582 - val_loss: 0.7096\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9910 - loss: 0.0275 - val_accuracy: 0.8552 - val_loss: 0.7060\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9970 - loss: 0.0128 - val_accuracy: 0.8116 - val_loss: 0.9496\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9877 - loss: 0.0347 - val_accuracy: 0.8476 - val_loss: 0.7392\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9955 - loss: 0.0157 - val_accuracy: 0.8402 - val_loss: 0.7012\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9930 - loss: 0.0240 - val_accuracy: 0.8236 - val_loss: 0.7344\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9956 - loss: 0.0165 - val_accuracy: 0.8462 - val_loss: 0.7844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code trains the LSTM model on the training data (x_train and y_train) for 10 epochs with a batch size of 32. The fit function initiates the training process, where the model iteratively adjusts its weights to minimize the binary cross-entropy loss and improve accuracy. The epochs=10 parameter specifies that the model will go through the entire training dataset 10 times. The batch_size=32 indicates that the model will process 32 samples in each training step before updating its weights. Additionally, the validation_split=0.2 parameter reserves 20% of the training data for validation during training, allowing the model to evaluate its performance on unseen data at the end of each epoch. The result of the training process, including the loss and accuracy metrics, is stored in the history object for later analysis, such as visualizing the training progress."
      ],
      "metadata": {
        "id": "_xcDpOgzfqvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "test_loss,test_acc=model.evaluate(x_test,y_test)\n",
        "print('Test accuracy:',test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqo1ya-bBS55",
        "outputId": "0aa56993-eaee-487d-afdc-ccece41d65e5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8589 - loss: 0.5785\n",
            "Test accuracy: 0.8589199781417847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code evaluates the performance of the trained LSTM model on the test dataset (x_test and y_test) using the model.evaluate function. It calculates two key metrics: the test loss and the test accuracy. The test loss is computed using the binary cross-entropy loss function, which quantifies how well the model's predictions match the true labels for the test set. The test accuracy represents the proportion of correct predictions made by the model on the test set, indicating how well the model generalizes to unseen data. After evaluation, the test accuracy is printed to the console, providing a measure of the model's effectiveness in classifying the sentiment of the test reviews."
      ],
      "metadata": {
        "id": "kpNkVioyf62H"
      }
    }
  ]
}