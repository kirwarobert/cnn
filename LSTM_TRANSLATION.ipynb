{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirwarobert/cnn/blob/main/LSTM_TRANSLATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "metadata": {
        "id": "Owu9iBlP9yk8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code attempts to import necessary libraries for deep learning and data manipulation but contains a small error. It imports numpy as pd, which is unconventional because pd is typically used as an alias for the Pandas library, not NumPy. NumPy is usually imported as np and is used for numerical operations and handling arrays. Additionally, the code imports keras from TensorFlow and the layers module from tensorflow.keras, which are essential for building and training deep learning models. keras provides high-level APIs for defining neural networks, while layers includes building blocks like Dense, Conv2D, and LSTM, which are used to create the architecture of the neural network. This code sets up the libraries needed for building machine learning models using TensorFlow and Keras."
      ],
      "metadata": {
        "id": "cEfrqOZ-uU0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "# Load the IMDB dataset\n",
        "max_features = 10000 # Number of words to consideras features\n",
        "max_len = 200 # Trim reviews after ths number of words\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=max_features)\n",
        "# Pad sequences to a fixed length\n",
        "X = train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLwHuiAm_fMg",
        "outputId": "69440d03-c70e-4668-894c-6b1d9a04a602"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads and preprocesses the IMDB dataset for sentiment analysis using the Keras library. First, it sets two parameters: max_features = 10000, which limits the vocabulary to the 10,000 most frequent words in the dataset, and max_len = 200, which trims or pads each review to a fixed length of 200 words. The dataset is then loaded using keras.datasets.imdb.load_data(num_words=max_features), which retrieves the training (X_train, y_train) and test (X_test, y_test) data. The reviews are represented as sequences of integers, with each integer corresponding to a word in the vocabulary. To ensure that all reviews have the same length, the code applies padding to both the training and test sequences using keras.preprocessing.sequence.pad_sequences, which truncates reviews longer than 200 words and pads shorter reviews with zeros. This prepares the data for input into a neural network model, ensuring consistent input size."
      ],
      "metadata": {
        "id": "dhJoO9m-ujH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the GRU model\n",
        "model = keras.Sequential([\n",
        "    layers.Embedding(max_features, 128), # Added comma here\n",
        "    layers.GRU(128, dropout=0.2, recurrent_dropout=0.2), # Added comma here\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "]) # This is the correct closing parenthesis for the list"
      ],
      "metadata": {
        "id": "Lkp0XtQaAGH-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code builds a GRU (Gated Recurrent Unit) model for sentiment analysis using Keras' Sequential API. The model begins with an Embedding layer that converts integer sequences (representing words) into dense vectors of size 128, where max_features (10,000) determines the vocabulary size. The GRU layer follows, which has 128 units and incorporates dropout (0.2) to prevent overfitting by randomly setting a fraction of input units to zero during training, and recurrent_dropout (0.2) to apply dropout to the recurrent connections within the GRU. This helps the model generalize better by preventing it from memorizing the training data. Finally, a Dense layer with a single unit and a sigmoid activation function outputs a probability between 0 and 1, representing the likelihood that a review is positive (1) or negative (0). This model is designed to classify the sentiment of IMDB movie reviews. The code also correctly places commas and parentheses to define the layers properly."
      ],
      "metadata": {
        "id": "a6LUv1LiuxMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "model.fit(X, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ5Hc_WYAI4v",
        "outputId": "202e7546-bc15-4ff7-c6f7-d2870d080e8f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m511s\u001b[0m 641ms/step - accuracy: 0.6758 - loss: 0.5736 - val_accuracy: 0.8350 - val_loss: 0.3744\n",
            "Epoch 2/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 644ms/step - accuracy: 0.8706 - loss: 0.3202 - val_accuracy: 0.8790 - val_loss: 0.2931\n",
            "Epoch 3/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 643ms/step - accuracy: 0.9322 - loss: 0.1777 - val_accuracy: 0.8843 - val_loss: 0.2872\n",
            "Epoch 4/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 658ms/step - accuracy: 0.9614 - loss: 0.1126 - val_accuracy: 0.8726 - val_loss: 0.3459\n",
            "Epoch 5/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 649ms/step - accuracy: 0.9796 - loss: 0.0645 - val_accuracy: 0.8694 - val_loss: 0.4525\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fbd277a6790>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code compiles and trains the GRU model for sentiment analysis. First, the model.compile function is used to prepare the model for training, specifying the binary cross-entropy loss function, which is appropriate for binary classification tasks like sentiment analysis, and the Adam optimizer, a widely used optimization algorithm that adjusts the learning rate during training. The model will also track accuracy as a performance metric. Then, the model is trained using model.fit, where the training data (X and y_train) is fed into the model. The batch_size=32 means the model will process 32 samples before updating its weights, and the training will run for 5 epochs (full passes through the training dataset). The validation_data=(X_test, y_test) argument allows the model to evaluate its performance on the test set after each epoch, helping monitor overfitting and generalization. This setup trains the model to classify IMDB reviews as positive or negative."
      ],
      "metadata": {
        "id": "jjc8UdcIvDKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test,)\n",
        "print(f'Test loss: {loss:.4f}')\n",
        "print(f'Test accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "sGTzMD7nBRo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db095025-e29f-43c1-cec9-b3e77acff6c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 108ms/step - accuracy: 0.8659 - loss: 0.4645\n",
            "Test loss: 0.4525\n",
            "Test accuracy: 0.8694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code evaluates the performance of the trained GRU model on the test dataset (X_test and y_test) using the model.evaluate function. It calculates the loss and accuracy of the model's predictions on the test set. The loss represents how well the model's predictions align with the true labels, using binary cross-entropy as the loss function, while accuracy measures the proportion of correct predictions. After evaluation, the loss and accuracy values are printed to the console with a precision of four decimal places, providing a clear indication of the model's effectiveness in classifying the sentiment of IMDB reviews on unseen data."
      ],
      "metadata": {
        "id": "m7HBz3wPvRXg"
      }
    }
  ]
}